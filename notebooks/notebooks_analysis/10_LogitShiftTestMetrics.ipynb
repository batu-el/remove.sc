{"cells":[{"cell_type":"markdown","metadata":{"id":"03FcolRXu3D-"},"source":["# Install"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5752,"status":"ok","timestamp":1716343290641,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"UUFluepQu5S8","outputId":"2c86866f-fc7e-477d-d950-aaa1b4c15ca8"},"outputs":[],"source":["!pip install einops datasets jaxtyping better_abc fancy_einsum wandb netcal"]},{"cell_type":"markdown","metadata":{"id":"8trqjnsHBqft"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6302,"status":"ok","timestamp":1716344569679,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"JT1BvkRYBuFX","outputId":"93181b3c-2875-46cf-8f4e-75d378450748"},"outputs":[],"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","path_to_root = '/content/drive/My Drive/Colab Notebooks/BatuEl_Dissertation'\n","sys.path.append(path_to_root)\n","print(\"Drive mounted.\")\n","\n","data_path = path_to_root + '/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716344569680,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"bW4KeHLnDSuu"},"outputs":[],"source":["import torch\n","import tqdm\n","from reprshift.learning.algorithms import ERM\n","from reprshift.models.hparams import hparams_f\n","from reprshift.dataset.datasets import MultiNLI, CivilComments\n","from reprshift.dataset.dataloaders import InfiniteDataLoader, FastDataLoader\n","\n","from reprshift.models.model_param_maps import ERM_to_HookedEncoder, load_focal, load_groupdro, load_jtt, load_lff\n","from reprshift.models.HookedEncoderConfig import bert_config\n","\n","from transformer_lens2 import HookedEncoder, HookedTransformerConfig"]},{"cell_type":"markdown","metadata":{"id":"iKQdF7UeskXT"},"source":["# Define Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":803,"status":"ok","timestamp":1716343296819,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"0QRqS486smTP"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, average_precision_score,\n","                             balanced_accuracy_score, recall_score, brier_score_loss, log_loss, classification_report)\n","# import netcal.metrics\n","\n","\n","## From Subpopbench Repository ##\n","\n","\n","def predict_on_set(algorithm, loader, device):\n","    num_labels = loader.dataset.num_labels\n","\n","    ys, atts, gs, ps = [], [], [], []\n","\n","    algorithm.eval()\n","    with torch.no_grad():\n","        for _, x, y, a in loader:\n","            p = algorithm.predict(x.to(device))\n","            if p.squeeze().ndim == 1:\n","                p = torch.sigmoid(p).detach().cpu().numpy()\n","            else:\n","                p = torch.softmax(p, dim=-1).detach().cpu().numpy()\n","                if num_labels == 2:\n","                    p = p[:, 1]\n","\n","            ps.append(p)\n","            ys.append(y)\n","            atts.append(a)\n","            gs.append([f'y={yi},a={gi}' for c, (yi, gi) in enumerate(zip(y, a))])\n","\n","    return np.concatenate(ys, axis=0), np.concatenate(atts, axis=0), np.concatenate(ps, axis=0), np.concatenate(gs)\n","\n","def eval_metrics(algorithm, loader, device, thres=0.5):\n","    targets, attributes, preds, gs = predict_on_set(algorithm, loader, device)\n","    preds_rounded = preds >= thres if preds.squeeze().ndim == 1 else preds.argmax(1)\n","    label_set = np.unique(targets)\n","\n","    res = {}\n","    res['overall'] = {\n","        **binary_metrics(targets, preds_rounded, label_set),\n","        # **prob_metrics(targets, preds, label_set)\n","    }\n","    res['per_attribute'] = {}\n","    res['per_class'] = {}\n","    res['per_group'] = {}\n","\n","    for a in np.unique(attributes):\n","        mask = attributes == a\n","        res['per_attribute'][int(a)] = {\n","            **binary_metrics(targets[mask], preds_rounded[mask], label_set),\n","            # **prob_metrics(targets[mask], preds[mask], label_set)\n","        }\n","\n","    classes_report = classification_report(targets, preds_rounded, output_dict=True, zero_division=0.)\n","    res['overall']['macro_avg'] = classes_report['macro avg']\n","    res['overall']['weighted_avg'] = classes_report['weighted avg']\n","    for y in np.unique(targets):\n","        res['per_class'][int(y)] = classes_report[str(y)]\n","\n","    for g in np.unique(gs):\n","        mask = gs == g\n","        res['per_group'][g] = {\n","            **binary_metrics(targets[mask], preds_rounded[mask], label_set)\n","        }\n","\n","    res['adjusted_accuracy'] = sum([res['per_group'][g]['accuracy'] for g in np.unique(gs)]) / len(np.unique(gs))\n","    res['min_attr'] = pd.DataFrame(res['per_attribute']).min(axis=1).to_dict()\n","    res['max_attr'] = pd.DataFrame(res['per_attribute']).max(axis=1).to_dict()\n","    res['min_group'] = pd.DataFrame(res['per_group']).min(axis=1).to_dict()\n","    res['max_group'] = pd.DataFrame(res['per_group']).max(axis=1).to_dict()\n","    return res\n","\n","def binary_metrics(targets, preds, label_set=[0, 1], return_arrays=False):\n","    if len(targets) == 0:\n","        return {}\n","    res = {\n","        'accuracy': accuracy_score(targets, preds),\n","        'n_samples': len(targets)\n","    }\n","\n","    if len(label_set) == 2:\n","        CM = confusion_matrix(targets, preds, labels=label_set)\n","\n","        res['TN'] = CM[0][0].item()\n","        res['FN'] = CM[1][0].item()\n","        res['TP'] = CM[1][1].item()\n","        res['FP'] = CM[0][1].item()\n","        res['error'] = res['FN'] + res['FP']\n","\n","        if res['TP'] + res['FN'] == 0:\n","            res['TPR'] = 0\n","            res['FNR'] = 1\n","        else:\n","            res['TPR'] = res['TP']/(res['TP']+res['FN'])\n","            res['FNR'] = res['FN']/(res['TP']+res['FN'])\n","\n","        if res['FP'] + res['TN'] == 0:\n","            res['FPR'] = 1\n","            res['TNR'] = 0\n","        else:\n","            res['FPR'] = res['FP']/(res['FP']+res['TN'])\n","            res['TNR'] = res['TN']/(res['FP']+res['TN'])\n","\n","        res['pred_prevalence'] = (res['TP'] + res['FP']) / res['n_samples']\n","        res['prevalence'] = (res['TP'] + res['FN']) / res['n_samples']\n","    else:\n","        CM = confusion_matrix(targets, preds, labels=label_set)\n","        res['TPR'] = recall_score(targets, preds, labels=label_set, average='macro', zero_division=0.)\n","\n","    if len(np.unique(targets)) > 1:\n","        res['balanced_acc'] = balanced_accuracy_score(targets, preds)\n","    if return_arrays:\n","        res['targets'] = targets\n","        res['preds'] = preds\n","    return res\n","\n","# def prob_metrics(targets, preds, label_set, return_arrays=False):\n","#     if len(targets) == 0:\n","#         return {}\n","\n","#     res = {\n","#         'AUROC_ovo': roc_auc_score(targets, preds, multi_class='ovo', labels=label_set),\n","#         'BCE': log_loss(targets, preds, eps=1e-6, labels=label_set),\n","#         'ECE': netcal.metrics.ECE().measure(preds, targets)\n","#     }\n","\n","#     # happens when you predict a class, but there are no samples with that class in the dataset\n","#     try:\n","#         res['AUROC'] = roc_auc_score(targets, preds, multi_class='ovr', labels=label_set)\n","#     except:\n","#         res['AUROC'] = roc_auc_score(targets, preds, multi_class='ovo', labels=label_set)\n","\n","#     if len(set(targets)) == 2:\n","#         res['AUPRC'] = average_precision_score(targets, preds, average='macro')\n","#         res['brier'] = brier_score_loss(targets, preds)\n","\n","#     if return_arrays:\n","#         res['targets'] = targets\n","#         res['preds'] = preds\n","\n","#     return res"]},{"cell_type":"markdown","metadata":{"id":"_kycbc_ci9qU"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31057,"status":"ok","timestamp":1716343327875,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"qa7s8EU3i-2g","outputId":"ff77e332-b32f-453b-bfa4-14284eba361f"},"outputs":[],"source":["DATASET = 'MultiNLI'  # 'CivilComments' , 'MultiNLI'\n","\n","if DATASET == 'MultiNLI':\n","    NUM_CLASSES = 3\n","    NUM_ATTRIBUTES = 2\n","    # train_dataset = MultiNLI(data_path, 'tr', hparams)\n","    # val_dataset = MultiNLI(data_path, 'va', hparams=hparams_f('ERM'))\n","    te_dataset = MultiNLI(data_path, 'te', hparams=hparams_f('ERM'))\n","    models_path = path_to_root + '/models/models_mnli'\n","    representations_path = path_to_root + '/representations/representations_mnli'\n","    print(DATASET)\n","elif DATASET  == 'CivilComments':\n","    NUM_CLASSES = 2\n","    NUM_ATTRIBUTES = 8\n","    # train_dataset = CivilComments(data_path, 'tr', hparams, granularity=\"fine\")\n","    # val_dataset = CivilComments(data_path, 'va', hparams=hparams_f('ERM'))\n","    te_dataset = CivilComments(data_path, 'te', hparams=hparams_f('ERM'))\n","    models_path = path_to_root + '/models/models_civilcomments'\n","    representations_path = path_to_root + '/representations/representations_civilcomments'\n","    print(DATASET)\n","else:\n","    print('Dataset Not Implemented')"]},{"cell_type":"markdown","metadata":{"id":"iA9bq1AmbnOK"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":504619,"status":"ok","timestamp":1716348208615,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"B1hcEbSzborX","outputId":"35999447-35c1-4de8-a2c2-925fb9a386cb"},"outputs":[],"source":["LogitShiftEdits = {0:1.6, 1:2.1, 2:1.7}\n","\n","for SEED in [0,1,2]:\n","    print('SEED: ', SEED)\n","    valmetrics = torch.load(path_to_root + f'/results/ValidationMetrics/clean_val_results.pth')\n","    CURRENT_BEST_EPOCH = valmetrics[DATASET][SEED]['A Selection']\n","\n","    ### MODELS ###\n","    MODELS  = {'pretrained': {'path': models_path + '/00_randominit/',  'load_f': lambda x: x, 'epoch':0,},\n","              'erm': {'path': models_path + '/01_erm/',  'load_f': lambda x: x, 'epoch':CURRENT_BEST_EPOCH['erm'],},\n","              'groupdro': {'path': models_path + '/03_groupdro/',  'load_f': load_groupdro, 'epoch':CURRENT_BEST_EPOCH['groupdro'],},\n","              'jtt': {'path': models_path + '/06_jtt/',  'load_f': load_jtt, 'epoch':CURRENT_BEST_EPOCH['jtt'],},\n","              'lff': {'path': models_path + '/07_lff/',  'load_f': load_lff, 'epoch':CURRENT_BEST_EPOCH['lff'],},\n","              'focal': {'path': models_path + '/15_focal/',  'load_f': lambda x: x, 'epoch':CURRENT_BEST_EPOCH['focal'],},}\n","\n","    ### Load Statedict ###\n","    algorithm_name = 'erm'\n","    state_dict_PATH = MODELS[algorithm_name]['path']\n","    load_f =  MODELS[algorithm_name]['load_f']\n","    epoch = MODELS[algorithm_name]['epoch']\n","    algorithm_state_dict_PATH = state_dict_PATH + f'seed{SEED}/sd_epoch{epoch}.pth'\n","    sd = load_f(torch.load(algorithm_state_dict_PATH))\n","\n","    ### Edit State Dict ###\n","    print(sd['network.1.classifier.bias'])\n","    sd['network.1.classifier.bias'] = sd['network.1.classifier.bias'] - torch.tensor([LogitShiftEdits[SEED], 0, 0]).cuda()\n","    print(sd['network.1.classifier.bias'])\n","\n","    ### Initialize ERM Model ###\n","    hparams = hparams_f('ERM')\n","    algorithm = ERM(num_classes=NUM_CLASSES, num_attributes=NUM_ATTRIBUTES, hparams=hparams)\n","    algorithm.load_state_dict(sd)\n","\n","    # from reprshift.utils import eval_helper\n","    te_metrics = {}\n","    for algorithm_name in tqdm.tqdm(['erm']):\n","        print(algorithm_name)\n","        te_loader = FastDataLoader(  dataset=te_dataset,\n","                                      batch_size=128,\n","                                      num_workers=1)\n","        te_metrics[algorithm_name] =  eval_metrics(algorithm.cuda(), te_loader, 'cuda')\n","\n","    temetrics_PATH = path_to_root + f'/results/ModelEdits/{DATASET}_logitshift_seed{SEED}.pth'\n","    torch.save(te_metrics, temetrics_PATH)"]},{"cell_type":"markdown","metadata":{"id":"5sxMUDZj6Flv"},"source":["# Load Test Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1218,"status":"ok","timestamp":1716349915793,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"VG_WaVoBQi6v"},"outputs":[],"source":["import torch\n","\n","DATASET = 'MultiNLI'\n","TestTables = []\n","\n","for SEED in [0,1,2]:\n","    te_metrics_load = torch.load(path_to_root + f'/results/ModelEdits/{DATASET}_logitshift_seed{SEED}.pth')\n","    group_keys = te_metrics_load['erm']['per_group'].keys()\n","    d = {algo_key: {epoch : {group_key: te_metrics_load[algo_key]['per_group'][group_key]['accuracy'].round(4) * 100 for group_key in group_keys}} for algo_key in ['erm']}\n","    df = pd.concat({algo_key:pd.DataFrame(d[algo_key]) for algo_key in d.keys()} , axis=1)\n","    df.columns = df.columns.droplevel(level=1)\n","    overall = pd.DataFrame(index=['Overall'], data={algo_key: te_metrics_load[algo_key]['overall']['accuracy'].round(4) * 100 for algo_key in ['erm']})\n","    df = pd.concat([overall, df])\n","    TestTable = df[['erm']]\n","    TestTable.to_csv(path_to_root + f'/results/ModelEdits/{DATASET}_logitshift_seed{SEED}_TestTable.csv')\n","    TestTables.append(TestTable)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1079,"status":"ok","timestamp":1716348229817,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"AD69X61EJkIZ"},"outputs":[],"source":["# TestTables[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716350510503,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"3iyQOgt3QXC0","outputId":"e4071977-9e2f-4173-96b8-5cf46817096e"},"outputs":[],"source":["### Test Table ###\n","dfs = TestTables\n","dfs = [pd.DataFrame(TestTables[i].drop('Overall').mean()) for i in range(2)] # To Calculate average accuracy\n","stacked_dfs = np.stack(dfs)\n","df_mean_values = np.mean(stacked_dfs, axis=0)\n","df_std_values = np.std(stacked_dfs, axis=0)\n","df_mean = pd.DataFrame(df_mean_values, columns=dfs[0].columns, index=dfs[0].index)\n","df_std = pd.DataFrame(df_std_values, columns=dfs[0].columns, index=dfs[0].index)\n","df_std.round(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":390,"status":"ok","timestamp":1716347529365,"user":{"displayName":"Batu El","userId":"11666366648103508022"},"user_tz":-60},"id":"-SzFSV726HBG"},"outputs":[],"source":["# temetrics_PATH = path_to_root + f'/results/{DATASET}_temetrics_seed{SEED}.pth'\n","# print(temetrics_PATH)\n","# te_metrics_load = torch.load(temetrics_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h0l24fLs370"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNbgQGmAA8Jt+Gcaz8w7hJZ","collapsed_sections":["03FcolRXu3D-"],"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
